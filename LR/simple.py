import matplotlib.pyplot as plt
import pandas as pd
import pylab as pl
import numpy as np
from sklearn.metrics import r2_score
from sklearn import linear_model

filename = "FuelConsumptionCo2.csv"
df = pd.read_csv(filename)

df.head() # display first few rows of the dataframe
df.describe() # displays summary statistics of the columns
# count: The number of non-null values in each column.
# mean: The arithmetic mean (average) of each column.
# std: The standard deviation of each column, indicating the spread of data points around the mean.
# min: The minimum value in each column.
# 25%: The first quartile (25th percentile) value.
# 50%: The second quartile or median (50th percentile) value.
# 75%: The third quartile (75th percentile) value.
# max: The maximum value in each column.

cdf = df[["ENGINESIZE", "CYLINDERS", "FUELCONSUMPTION_COMB", "CO2EMISSIONS"]] # create a dataframe with specific columns
cdf.head(9) # get first 9 rows


viz = cdf[['CYLINDERS','ENGINESIZE','CO2EMISSIONS','FUELCONSUMPTION_COMB']] #create another datafram from specific columns
viz.hist() # created the histogram 


# created a scatter plot where x axis is FUELCONSUMPTION_COMB and y axis is CO2EMISSIONS, each dot represents the specific fuel consumption and CO2 emissions value
# plt.scatter(cdf.FUELCONSUMPTION_COMB, cdf.CO2EMISSIONS,  color='blue')
# plt.xlabel("FUELCONSUMPTION_COMB") # set label of x axis
# plt.ylabel("Emission") # set label y axis


# plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,  color='red')
# plt.xlabel("Engine size")
# plt.ylabel("Emission")


"""
This generates an array of random values between 0 and 1, with the length equal to the number of rows in the DataFrame 
"""
msk = np.random.rand(len(df)) < 0.8 # each randomly generated value is less than 0.8, 80 % will be true and 20 % will be false
train = cdf[msk] # selecting rows from dataframe, where the corresponding value in the msk array is True
test = cdf[~msk] # ~ symbol is a bitwise NOT operator, where the corresponding value in the inverted msk array is true

plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue')
plt.xlabel("Engine size")
plt.ylabel("Emission")


regr = linear_model.LinearRegression() # create a linear regression instance, simple linear regression model

"""
train[['ENGINESIZE']] ->  selects the ENGINESIZE column from the training dataframe, which contains independent variables 
np.asanyarray         -> it will convert the columns into array, array are compatible with scikit learn models

train_x and train_y are the array which contains independent and dependent variables respectivly.
"""

train_x = np.asanyarray(train[['ENGINESIZE']])
train_y = np.asanyarray(train[['CO2EMISSIONS']]) # dependent variable

#It calculates the best-fit line that minimizes the sum of squared differences between the predicted values and the actual values of the dependent variable
regr.fit(train_x, train_y) #  fits the model to the training data
# calculated slope (coefficient) and intercept of the best-fit line

# The coefficients
print ('Coefficients: ', regr.coef_)
print ('Intercept: ',regr.intercept_)

plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue')
"""
train_x  -> independent variable, x-axis values
regr.coef_[0][0]*train_x + regr.intercept_[0] --> This is the equation of the best-fit line generated by the linear regression model

This equation calculates the predicted values of the dependent variable (CO2 emissions) based on the independent variable (engine size)
"""
plt.plot(train_x, regr.coef_[0][0]*train_x + regr.intercept_[0], '-r')
plt.xlabel("Engine size")
plt.ylabel("Emission")

#plt.show()

# create a numpy array for dependent and independent variables from test dataframe
test_x = np.asanyarray(test[['ENGINESIZE']])
test_y = np.asanyarray(test[['CO2EMISSIONS']])
"""
this uses the trained LR model to predict the dependent variable based of independent variable
"""
test_y_ = regr.predict(test_x)

# mean absolute error(MAE) measures the average absolute difference between predicted and actual values
print("Mean absolute error: %.2f" % np.mean(np.absolute(test_y_ - test_y)))
# mean squared error(MSE), MSE measures the average squared difference between predicted and actual values
print("Residual sum of squares (MSE): %.2f" % np.mean((test_y_ - test_y) ** 2))
#The R-squared score indicates how well the model's predictions explain the variability of the actual data, It ranges from 0 to 1, where 1 indicates a perfect fit
print("R2-score: %.2f" % r2_score(test_y , test_y_) )